{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0948cc75-eab4-4c68-919f-ce894640f1d8",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88775e-458f-4f98-bb05-695532eb69fd",
   "metadata": {},
   "source": [
    "#### 1.1 Install Dependencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d617b8-b4f9-44ce-896c-145b83f60ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/ashiq/anaconda3/lib/python3.11/site-packages (2.14.0)\n",
      "Requirement already satisfied: opencv-python in /Users/ashiq/anaconda3/lib/python3.11/site-packages (4.8.1.78)\n",
      "Requirement already satisfied: matplotlib in /Users/ashiq/anaconda3/lib/python3.11/site-packages (3.7.2)\n",
      "Requirement already satisfied: tensorflow-macos==2.14.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (4.24.4)\n",
      "Requirement already satisfied: setuptools in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.59.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.14.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/ashiq/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ed091-c04f-4ecc-a40c-ace30b1336b9",
   "metadata": {},
   "source": [
    "#### 1.2 Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e82e54c6-b432-4d38-bba6-d7dbe4692c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ac56a2-464f-43d4-818c-c94267b4950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow dependencies - Functional API\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8566974a-10bb-4da0-a3bc-d18ada435682",
   "metadata": {},
   "source": [
    "#### 1.3 Create Folder Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69786187-1ed3-4769-bc2d-28fb94983c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup paths\n",
    "POS_PATH = os.path.join('data', 'positive') #data to be verified\n",
    "NEG_PATH = os.path.join('data', 'negative') #data to be verified\n",
    "ANC_PATH = os.path.join('data', 'anchor') #input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3f272a-5bce-44a3-abf5-72f52f3714da",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(POS_PATH)\n",
    "os.makedirs(NEG_PATH)\n",
    "os.makedirs(ANC_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51c6d7-e57f-423a-890e-04859a104bbc",
   "metadata": {},
   "source": [
    "### 2. Collect Positives and Anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d653dbd2-7b78-418a-9bd9-38aeac6a2570",
   "metadata": {},
   "source": [
    "#### 2.1 Untar Labelled Faces in the Wild Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d396ab25-e3d0-44d9-a2ad-f9c48c41c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move LFW images to the following repository data/negative\n",
    "\n",
    "for directory in os.listdir('lfw'):\n",
    "    directory_path = os.path.join('lfw', directory)\n",
    "    if os.path.isdir(directory_path):\n",
    "        for file in os.listdir(directory_path):\n",
    "            EX_PATH = os.path.join('lfw', directory, file)\n",
    "            NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "            os.replace(EX_PATH,NEW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b864b33-8b97-45ba-8c6c-86cdeb3cc097",
   "metadata": {},
   "source": [
    "#### 2.2 Collect Positive and Anchor Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "826bc6f4-fc04-4ed5-b53c-1f7d0d3b16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import uuid library to generate unique image names\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa5d67eb-e87d-427c-a1a6-8df932a45254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 13:08:56.234 python[86122:16560541] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    #  image resizing\n",
    "    frame = frame[550: 550+250, 900: 900+250, :]\n",
    "\n",
    "    # collect image for anchors\n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        imgname = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.waitKey(1)\n",
    "        cv2.imwrite(imgname, frame)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    # collect image for positives\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        imgname = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.waitKey(1)\n",
    "        cv2.imwrite(imgname, frame)\n",
    "        cv2.waitKey(1)\n",
    "    \n",
    "    #show image back to screen\n",
    "    cv2.imshow('Image Collection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "# Release the webcam\n",
    "cap.release()\n",
    "# Close the image show frame\n",
    "cv2.destroyAllWindows()\n",
    "# need to have waitkey again after destroying all windows\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f49c57-2322-47f7-a573-061e2e221ca9",
   "metadata": {},
   "source": [
    "### 3. Load and Preprocess images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad513fc-6bbe-4839-95eb-57e047269050",
   "metadata": {},
   "source": [
    "#### 3.1 Get Image Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3af563-fb65-4ebf-8be7-fbea69fecbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing for pre-processing pipeline\n",
    "\n",
    "# change / to \\ if using windows\n",
    "anchor = tf.data.Dataset.list_files(ANC_PATH+'/*.jpg').take(300)\n",
    "positive = tf.data.Dataset.list_files(POS_PATH+'/*.jpg').take(300)\n",
    "negative = tf.data.Dataset.list_files(NEG_PATH+'/*.jpg').take(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4845d-ddb3-4906-8c33-73e357eb23e3",
   "metadata": {},
   "source": [
    "#### 3.2 Preprocessing - Scale and Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b608837-56fa-420b-8701-8b94da3f217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    # read in image from filepath\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    # load in the image\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    # preprocessing: resize acc to Siamese network paper\n",
    "    img = tf.image.resize(img, (100,100))\n",
    "    # Scale iage to be between 0 and 1\n",
    "    img = img / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6281f0-9fe9-4466-916f-4172b90909e5",
   "metadata": {},
   "source": [
    "#### 3.3 Create Lablelled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "685266c2-f02b-4c83-a9e6-efe95a3e56f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fdfad2-ddbf-42f4-8a39-18f41a0c5462",
   "metadata": {},
   "source": [
    "#### 3.4 Build Train and Test Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "337cb094-6b1e-4905-b1cf-ae70b27932eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    return (preprocess(input_img), preprocess(validation_img), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "388e62cf-2f78-4484-92d4-6b8bdbcb0da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloader pipelin\n",
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad693b72-e48a-452f-ac70-d13e44e12c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training partition\n",
    "\n",
    "train_data = data.take(round(len(data)*0.7))\n",
    "train_data = train_data.batch(16) # batches of 16\n",
    "train_data = train_data.prefetch(8) #starts preprocessing the next set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9642e2b3-84b1-4d9e-a299-c2d14a96e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for understanding purposes\n",
    "\n",
    "train_samples = train_data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31a7d7ca-88ad-461c-9a2d-156048deaed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "116806d9-b02a-4786-a51b-46a3db51a062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sample[0]) #number of images in one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df7b807c-d534-4ff7-9a14-ccdc3760d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing partition\n",
    "\n",
    "test_data = data.skip(round(len(data)*0.7))\n",
    "test_data = test_data.take(round(len(data)*0.3))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f2252-3c0c-4d78-9eba-c35436c689fa",
   "metadata": {},
   "source": [
    "### 4. Model Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081fc21b-ab76-45a8-a2ca-c9ffb8f7cc4f",
   "metadata": {},
   "source": [
    "- Build an embedding layer\n",
    "- Create an L1 Distance layer\n",
    "- Compile the Siamese Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c279f6-c385-445f-b010-108fa091ecaa",
   "metadata": {},
   "source": [
    "#### 4.1 Build Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "168d0bfb-bd74-4e45-a56f-1d786f00589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding():\n",
    "    inp = Input(shape=(100,100,3), name='input_image')\n",
    "\n",
    "    # First block\n",
    "    c1 = Conv2D(64, (10,10), activation='relu')(inp)\n",
    "    m1 = MaxPooling2D(64,(2,2), padding='same')(c1)\n",
    "\n",
    "    # Second block\n",
    "    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D(64,(2,2), padding='same')(c2)\n",
    "\n",
    "    # Third block\n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64,(2,2), padding='same')(c3)\n",
    "\n",
    "    # Final block\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "    \n",
    "    \n",
    "    return Model(inputs=[inp], outputs=[d1], name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da76bb52-ed54-4804-9499-cc9625edc15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = make_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7220e63-264e-4989-867e-97402f292fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 91, 91, 64)        19264     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 46, 46, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 40, 40, 128)       401536    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 20, 20, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 17, 17, 128)       262272    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 9, 9, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 256)         524544    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38960448 (148.62 MB)\n",
      "Trainable params: 38960448 (148.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1d087-eabd-4c63-92cf-ab721e19984c",
   "metadata": {},
   "source": [
    "#### 4.2 Build Distance Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "419b2c8a-00ea-45f8-a2fd-4b691c340f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese L1 Distance Class\n",
    "\n",
    "class L1Dist(Layer):\n",
    "\n",
    "    # Init Method - Inheritance\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    # Similarity Calculation\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30316806-5315-4624-84f9-6ac1ac7d9790",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = L1Dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a097460d-9f33-40d2-a451-de9b344f469b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.L1Dist at 0x16bad0d90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337b4258-5c42-4b85-aba2-01bea26b3662",
   "metadata": {},
   "source": [
    "#### 4.3 Make Siamese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "846ccb90-338a-4ea7-95e7-600fb4a6c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "\n",
    "    # Anchor image\n",
    "    input_image = Input(name='input_img', shape=(100,100,3))\n",
    "\n",
    "    # Validation image\n",
    "    validation_image = Input(name='validation_img', shape=(100,100,3))\n",
    "\n",
    "    # Combine siamese distance components\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "\n",
    "    # Classification Layer\n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "\n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaec937a-4886-459e-b18d-e8b03ead231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02666bde-1094-4dcd-81bb-356b56782c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_img (InputLayer)      [(None, 100, 100, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " validation_img (InputLayer  [(None, 100, 100, 3)]        0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)      (None, 4096)                 3896044   ['input_img[0][0]',           \n",
      "                                                          8          'validation_img[0][0]']      \n",
      "                                                                                                  \n",
      " distance (L1Dist)           (None, 4096)                 0         ['embedding[0][0]',           \n",
      "                                                                     'embedding[1][0]']           \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 1)                    4097      ['distance[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38964545 (148.64 MB)\n",
      "Trainable params: 38964545 (148.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d6dbf-e94e-44a4-9653-40bae30501ee",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0cbf7a-f7c7-46ba-b437-ba2f063369b5",
   "metadata": {},
   "source": [
    "#### 5.1 Setup Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f97e86d-e9a9-48bf-912d-3207f9159fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_loss = tf.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a0efbb9-074e-4a6b-9a8b-ddb319a09fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, \n",
    "# please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`\n",
    "opt = tf.keras.optimizers.legacy.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd7179b-c5c9-41ea-addb-2ac6b2424a5c",
   "metadata": {},
   "source": [
    "#### 5.2 Establish Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f07cf98c-b57f-46b2-b737-0ecfcea16c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints' \n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf8f6f1-f307-49e6-98dd-c99e2d7a35ff",
   "metadata": {},
   "source": [
    "#### 5.3 Build Train Step Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63c7fe9-4fac-4621-aaaa-0e78f761a3a3",
   "metadata": {},
   "source": [
    "Basic Flow for training on one batch is as follows:\n",
    "1. Make a prediction\n",
    "2. Calculate loss\n",
    "3. Derive gradients\n",
    "4. Calculate new weights and apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bc1e497-8732-4d50-bd81-e9a4794c647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function # wrapping the function inside the tf.function decorator\n",
    "def train_step(batch):\n",
    "    \n",
    "    with tf.GradientTape() as tape: #helps to capture gradients from our NN\n",
    "        # Get Anchor and positive/negative image\n",
    "        X = batch[:2]\n",
    "        # Get label\n",
    "        y = batch[2]\n",
    "\n",
    "        # Forward pass\n",
    "        yhat = siamese_model(X, training=True)\n",
    "        # Calculate binary loss\n",
    "        loss = binary_cross_loss(y, yhat)\n",
    "    print(loss)\n",
    "\n",
    "    # Calculate gradients\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "\n",
    "    # Calculate updated weights and apply to siamese model\n",
    "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "    return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daee407-3a3c-4648-869d-35104d0fd64e",
   "metadata": {},
   "source": [
    "#### 5.4 Build Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4515d2d1-39cb-49a1-8780-7515fbf815cd",
   "metadata": {},
   "source": [
    "While the train_step function was focused on training for one batch, the loop here will be used to iterate over every batch in the dataset.Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "984230db-4ef3-4627-a7cf-990f57037b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, EPOCHS):\n",
    "    # Loop through epochs\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(train_data))\n",
    "\n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(train_data):\n",
    "            # Run train step here\n",
    "            train_step(batch)\n",
    "            progbar.update(idx + 1)\n",
    "\n",
    "        # save checkpoints\n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eda0b7-9562-4f55-95ad-26e9b57fffd9",
   "metadata": {},
   "source": [
    "#### 5.5 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d395a97-acb5-473c-a26b-963ff565d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a2d6378-a6b4-4535-94de-fca857ce71a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1/50\n",
      "Tensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "Tensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "26/27 [===========================>..] - ETA: 2sTensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "27/27 [==============================] - 79s 3s/step\n",
      "\n",
      " Epoch 2/50\n",
      "27/27 [==============================] - 89s 3s/step\n",
      "\n",
      " Epoch 3/50\n",
      "27/27 [==============================] - 87s 3s/step\n",
      "\n",
      " Epoch 4/50\n",
      "27/27 [==============================] - 88s 3s/step\n",
      "\n",
      " Epoch 5/50\n",
      "27/27 [==============================] - 90s 3s/step\n",
      "\n",
      " Epoch 6/50\n",
      "27/27 [==============================] - 107s 4s/step\n",
      "\n",
      " Epoch 7/50\n",
      "27/27 [==============================] - 95s 4s/step\n",
      "\n",
      " Epoch 8/50\n",
      "27/27 [==============================] - 92s 3s/step\n",
      "\n",
      " Epoch 9/50\n",
      "27/27 [==============================] - 93s 3s/step\n",
      "\n",
      " Epoch 10/50\n",
      "27/27 [==============================] - 89s 3s/step\n",
      "\n",
      " Epoch 11/50\n",
      "27/27 [==============================] - 103s 4s/step\n",
      "\n",
      " Epoch 12/50\n",
      "27/27 [==============================] - 103s 4s/step\n",
      "\n",
      " Epoch 13/50\n",
      "27/27 [==============================] - 102s 4s/step\n",
      "\n",
      " Epoch 14/50\n",
      "27/27 [==============================] - 97s 4s/step\n",
      "\n",
      " Epoch 15/50\n",
      "27/27 [==============================] - 94s 3s/step\n",
      "\n",
      " Epoch 16/50\n",
      "27/27 [==============================] - 95s 4s/step\n",
      "\n",
      " Epoch 17/50\n",
      "27/27 [==============================] - 92s 3s/step\n",
      "\n",
      " Epoch 18/50\n",
      "27/27 [==============================] - 94s 3s/step\n",
      "\n",
      " Epoch 19/50\n",
      "27/27 [==============================] - 91s 3s/step\n",
      "\n",
      " Epoch 20/50\n",
      "27/27 [==============================] - 90s 3s/step\n",
      "\n",
      " Epoch 21/50\n",
      "27/27 [==============================] - 96s 4s/step\n",
      "\n",
      " Epoch 22/50\n",
      "27/27 [==============================] - 99s 4s/step\n",
      "\n",
      " Epoch 23/50\n",
      "27/27 [==============================] - 102s 4s/step\n",
      "\n",
      " Epoch 24/50\n",
      "27/27 [==============================] - 96s 4s/step\n",
      "\n",
      " Epoch 25/50\n",
      "27/27 [==============================] - 94s 3s/step\n",
      "\n",
      " Epoch 26/50\n",
      "27/27 [==============================] - 97s 4s/step\n",
      "\n",
      " Epoch 27/50\n",
      "27/27 [==============================] - 96s 4s/step\n",
      "\n",
      " Epoch 28/50\n",
      "27/27 [==============================] - 95s 4s/step\n",
      "\n",
      " Epoch 29/50\n",
      "27/27 [==============================] - 97s 4s/step\n",
      "\n",
      " Epoch 30/50\n",
      "27/27 [==============================] - 97s 4s/step\n",
      "\n",
      " Epoch 31/50\n",
      "27/27 [==============================] - 141s 5s/step\n",
      "\n",
      " Epoch 32/50\n",
      "27/27 [==============================] - 154s 6s/step\n",
      "\n",
      " Epoch 33/50\n",
      "27/27 [==============================] - 112s 4s/step\n",
      "\n",
      " Epoch 34/50\n",
      "27/27 [==============================] - 107s 4s/step\n",
      "\n",
      " Epoch 35/50\n",
      "27/27 [==============================] - 127s 5s/step\n",
      "\n",
      " Epoch 36/50\n",
      "27/27 [==============================] - 156s 6s/step\n",
      "\n",
      " Epoch 37/50\n",
      "27/27 [==============================] - 143s 5s/step\n",
      "\n",
      " Epoch 38/50\n",
      "27/27 [==============================] - 150s 6s/step\n",
      "\n",
      " Epoch 39/50\n",
      "27/27 [==============================] - 163s 6s/step\n",
      "\n",
      " Epoch 40/50\n",
      "27/27 [==============================] - 153s 6s/step\n",
      "\n",
      " Epoch 41/50\n",
      "27/27 [==============================] - 114s 4s/step\n",
      "\n",
      " Epoch 42/50\n",
      "27/27 [==============================] - 110s 4s/step\n",
      "\n",
      " Epoch 43/50\n",
      "27/27 [==============================] - 143s 5s/step\n",
      "\n",
      " Epoch 44/50\n",
      "27/27 [==============================] - 134s 5s/step\n",
      "\n",
      " Epoch 45/50\n",
      "27/27 [==============================] - 103s 4s/step\n",
      "\n",
      " Epoch 46/50\n",
      "27/27 [==============================] - 113s 4s/step\n",
      "\n",
      " Epoch 47/50\n",
      "27/27 [==============================] - 112s 4s/step\n",
      "\n",
      " Epoch 48/50\n",
      "27/27 [==============================] - 111s 4s/step\n",
      "\n",
      " Epoch 49/50\n",
      "27/27 [==============================] - 113s 4s/step\n",
      "\n",
      " Epoch 50/50\n",
      "27/27 [==============================] - 111s 4s/step\n"
     ]
    }
   ],
   "source": [
    "train(train_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e64d1-52a7-425c-96ae-14555f5c26b5",
   "metadata": {},
   "source": [
    "### 6. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f868d46-cabb-4ebe-b7e4-b58531a8e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metric calculations\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0f9f2923-994a-4a6b-8b81-d39a5e45bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_metric = Recall()\n",
    "precision_metric = Precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6949d682-46ca-4dbe-99d8-31adf5294e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 859ms/step\n",
      "1/1 [==============================] - 1s 679ms/step\n",
      "1/1 [==============================] - 1s 686ms/step\n",
      "1/1 [==============================] - 1s 706ms/step\n",
      "1/1 [==============================] - 1s 678ms/step\n",
      "1/1 [==============================] - 1s 698ms/step\n",
      "1/1 [==============================] - 1s 778ms/step\n",
      "1/1 [==============================] - 1s 684ms/step\n",
      "1/1 [==============================] - 1s 702ms/step\n",
      "1/1 [==============================] - 1s 716ms/step\n",
      "1/1 [==============================] - 1s 732ms/step\n",
      "1/1 [==============================] - 0s 222ms/step\n"
     ]
    }
   ],
   "source": [
    "test_batches = list(test_data)\n",
    "\n",
    "# Iterate over the entire test dataset\n",
    "for test_input, test_val, y_true in test_batches:\n",
    "    y_hat = siamese_model.predict([test_input, test_val])\n",
    "    y_hat = [1 if prediction > 0.5 else 0 for prediction in y_hat]\n",
    "    \n",
    "    # Update the metric with the true labels and predicted labels\n",
    "    recall_metric.update_state(y_true, y_hat)\n",
    "    precision_metric.update_state(y_true, y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "89c46da9-8c62-496a-8400-d58b778dbe58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the final recall result\n",
    "recall_result = recall_metric.result().numpy()\n",
    "recall_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6fd0bebc-e0e2-4ae3-994c-8f9c3009fc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the final precision result\n",
    "precision_result = precision_metric.result().numpy()\n",
    "precision_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f4d7d-bfb7-419a-b2ac-66a9480bc01d",
   "metadata": {},
   "source": [
    "### 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9a458aa-6420-418a-9262-bcc35c684118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# h5 legacy file format, using .keras as high level\n",
    "siamese_model.save('siamesemodel.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3bff2b6-8027-41a4-a2e8-e08dfbc6dd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('siamesemodel.h5', custom_objects={'L1Dist': L1Dist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f175c7c4-42a6-4f63-8568-462d7bc537ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_img (InputLayer)      [(None, 100, 100, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " validation_img (InputLayer  [(None, 100, 100, 3)]        0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)      (None, 4096)                 3896044   ['input_img[0][0]',           \n",
      "                                                          8          'validation_img[0][0]']      \n",
      "                                                                                                  \n",
      " l1_dist_6 (L1Dist)          (None, 4096)                 0         ['embedding[0][0]',           \n",
      "                                                                     'embedding[1][0]']           \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 1)                    4097      ['l1_dist_6[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38964545 (148.64 MB)\n",
      "Trainable params: 38964545 (148.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7fd8a9-ded8-49ec-ad7d-f518d82d0985",
   "metadata": {},
   "source": [
    "### 8. Real Time Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab578afb-ce39-4661-87b0-6e92e4692135",
   "metadata": {},
   "source": [
    "#### 8.1 Verification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b453a7b6-d2c3-4553-9af7-cb5ee0484925",
   "metadata": {},
   "outputs": [],
   "source": [
    " def verify(model, detection_threshold, verification_threshold):\n",
    "     results = []\n",
    "     for image in os.listdir(os.path.join('application_data', 'verification_images')):\n",
    "         input_img = preprocess(os.path.join('application_data', 'input_image', 'input_image.jpg'))\n",
    "         validation_img = preprocess(os.path.join('application_data', 'verification_images', image))\n",
    "         \n",
    "         result = model.predict(list(np.expand_dims([input_img, validation_img], axis = 1)), verbose=0)\n",
    "         results.append(result)\n",
    "\n",
    "     # Detection Threshold: Metric above which a prediction is considered positive\n",
    "     detection = np.sum(np.array(results) > detection_threshold)\n",
    "     \n",
    "     # Verification Threshold: Proportion of positive predictions / total positive samples\n",
    "     verification = detection / len(os.listdir(os.path.join('application_data', 'verification_images')))\n",
    "     verified = verification > verification_threshold\n",
    "\n",
    "     return results, verified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d99fe-8afb-4dff-98dd-b0b25b6aaab6",
   "metadata": {},
   "source": [
    "#### 8.2 OpenCV Real Time Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53cf5b4b-2130-4df9-a2c5-79a701118ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 07:57:09.854 python[19414:17763747] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__DecodeJpeg_device_/job:localhost/replica:0/task:0/device:CPU:0}} Unknown image file format. One of JPEG, PNG, GIF, BMP required. [Op:DecodeJpeg]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimwrite(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication_data\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_image\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m), frame)\n\u001b[1;32m     14\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     results, verified \u001b[38;5;241m=\u001b[39m verify( model, \u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(verified)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m&\u001b[39m  \u001b[38;5;241m0XFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m, in \u001b[0;36mverify\u001b[0;34m(model, detection_threshold, verification_threshold)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication_data\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverification_images\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m      4\u001b[0m     input_img \u001b[38;5;241m=\u001b[39m preprocess(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication_data\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_image\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m     validation_img \u001b[38;5;241m=\u001b[39m preprocess(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication_data\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverification_images\u001b[39m\u001b[38;5;124m'\u001b[39m, image))\n\u001b[1;32m      7\u001b[0m     result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mexpand_dims([input_img, validation_img], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m byte_img \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_file(file_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# load in the image\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m img \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mdecode_jpeg(byte_img)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# preprocessing: resize acc to Siamese network paper\u001b[39;00m\n\u001b[1;32m      7\u001b[0m img \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mresize(img, (\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m100\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/gen_image_ops.py:1186\u001b[0m, in \u001b[0;36mdecode_jpeg\u001b[0;34m(contents, channels, ratio, fancy_upscaling, try_recover_truncated, acceptable_fraction, dct_method, name)\u001b[0m\n\u001b[1;32m   1184\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1186\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m decode_jpeg_eager_fallback(\n\u001b[1;32m   1187\u001b[0m       contents, channels\u001b[38;5;241m=\u001b[39mchannels, ratio\u001b[38;5;241m=\u001b[39mratio,\n\u001b[1;32m   1188\u001b[0m       fancy_upscaling\u001b[38;5;241m=\u001b[39mfancy_upscaling,\n\u001b[1;32m   1189\u001b[0m       try_recover_truncated\u001b[38;5;241m=\u001b[39mtry_recover_truncated,\n\u001b[1;32m   1190\u001b[0m       acceptable_fraction\u001b[38;5;241m=\u001b[39macceptable_fraction, dct_method\u001b[38;5;241m=\u001b[39mdct_method,\n\u001b[1;32m   1191\u001b[0m       name\u001b[38;5;241m=\u001b[39mname, ctx\u001b[38;5;241m=\u001b[39m_ctx)\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m   1193\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/gen_image_ops.py:1260\u001b[0m, in \u001b[0;36mdecode_jpeg_eager_fallback\u001b[0;34m(contents, channels, ratio, fancy_upscaling, try_recover_truncated, acceptable_fraction, dct_method, name, ctx)\u001b[0m\n\u001b[1;32m   1256\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [contents]\n\u001b[1;32m   1257\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m\"\u001b[39m, channels, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mratio\u001b[39m\u001b[38;5;124m\"\u001b[39m, ratio, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfancy_upscaling\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1258\u001b[0m fancy_upscaling, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtry_recover_truncated\u001b[39m\u001b[38;5;124m\"\u001b[39m, try_recover_truncated,\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macceptable_fraction\u001b[39m\u001b[38;5;124m\"\u001b[39m, acceptable_fraction, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdct_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, dct_method)\n\u001b[0;32m-> 1260\u001b[0m _result \u001b[38;5;241m=\u001b[39m _execute\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecodeJpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, inputs\u001b[38;5;241m=\u001b[39m_inputs_flat,\n\u001b[1;32m   1261\u001b[0m                            attrs\u001b[38;5;241m=\u001b[39m_attrs, ctx\u001b[38;5;241m=\u001b[39mctx, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n\u001b[1;32m   1263\u001b[0m   _execute\u001b[38;5;241m.\u001b[39mrecord_gradient(\n\u001b[1;32m   1264\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecodeJpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__DecodeJpeg_device_/job:localhost/replica:0/task:0/device:CPU:0}} Unknown image file format. One of JPEG, PNG, GIF, BMP required. [Op:DecodeJpeg]"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    #  image resizing\n",
    "    frame = frame[550: 550+250, 900: 900+250, :]\n",
    "\n",
    "    cv2.imshow('Verification', frame)\n",
    "\n",
    "    # Verification trigger\n",
    "    if cv2.waitKey(10) &  0XFF == ord('v'):\n",
    "        # save input image to application_data/input_image folder\n",
    "        cv2.imwrite(os.path.join('application_data', 'input_image', 'input_image.jpg'), frame)\n",
    "        cv2.waitKey(1)\n",
    "        results, verified = verify( model, 0.5,0.5)\n",
    "        print(verified)\n",
    "\n",
    "    if cv2.waitKey(10) &  0XFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam\n",
    "cap.release()\n",
    "# Close the image show frame\n",
    "cv2.destroyAllWindows()\n",
    "# need to have waitkey again after destroying all windows\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b39981ae-fa4f-4765-8a4c-f416f8a7dfd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Release the webcam\n",
    "cap.release()\n",
    "# Close the image show frame\n",
    "cv2.destroyAllWindows()\n",
    "# need to have waitkey again after destroying all windows\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "072e18f9-101e-4f8b-b359-02441677464d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.squeeze(results) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8bf6e4e-0da0-44d4-a799-3a00b2a6595a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "43/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d078d43-24f9-4a02-b61d-10d6f6bb041c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
